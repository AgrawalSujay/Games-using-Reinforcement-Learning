{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiY3piz5a6Ly",
        "outputId": "42a8a0c4-04a0-4d26-d2bb-78e62cd35b45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.26.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "pip install gym numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "if not hasattr(np, 'bool8'):\n",
        "    np.bool8 = np.bool_"
      ],
      "metadata": {
        "id": "q44XB63L3utL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Buia9E1V2D01"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YPnxzIc284e",
        "outputId": "f2746ba9-1c9c-4f6e-cccf-7f846bf2dc6e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('MountainCar-v0')"
      ],
      "metadata": {
        "id": "D0vhSyQj2IyA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_bins = (18, 14)  # position, velocity\n",
        "obs_space_low = env.observation_space.low\n",
        "obs_space_high = env.observation_space.high\n",
        "obs_bin_width = (obs_space_high - obs_space_low) / n_bins"
      ],
      "metadata": {
        "id": "DIACaBww2LOx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def discretize(obs):\n",
        "    return tuple(((obs - obs_space_low) / obs_bin_width).astype(int))"
      ],
      "metadata": {
        "id": "aAvxfpWX2QFc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.1      # Learning rate\n",
        "gamma = 0.99     # Discount factor\n",
        "epsilon = 1.0    # Initial exploration rate\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "n_episodes = 10000\n",
        "max_steps = 200"
      ],
      "metadata": {
        "id": "gMn_Jndo2SaU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_table = np.zeros(n_bins + (env.action_space.n,))\n",
        "\n",
        "rewards = []\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "    obs = env.reset()\n",
        "    if isinstance(obs, tuple):\n",
        "        obs = obs[0]\n",
        "\n",
        "    state = discretize(obs)\n",
        "    total_reward = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # Epsilon-greedy action selection\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])\n",
        "\n",
        "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        next_state = discretize(next_obs)\n",
        "\n",
        "        # Q-Learning update\n",
        "        best_next_action = np.argmax(q_table[next_state])\n",
        "        td_target = reward + gamma * q_table[next_state][best_next_action]\n",
        "        td_error = td_target - q_table[state][action]\n",
        "        q_table[state][action] += alpha * td_error\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Decay epsilon\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "    if (episode + 1) % 1000 == 0:\n",
        "        print(f\"Episode {episode + 1}: Average Reward: {np.mean(rewards[-1000:])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTbu6Vji2a8r",
        "outputId": "bb508c4c-8f08-4cc7-a2fc-c6a9d4759c75"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1000: Average Reward: -197.483\n",
            "Episode 2000: Average Reward: -189.302\n",
            "Episode 3000: Average Reward: -172.82\n",
            "Episode 4000: Average Reward: -146.123\n",
            "Episode 5000: Average Reward: -145.52\n",
            "Episode 6000: Average Reward: -148.158\n",
            "Episode 7000: Average Reward: -143.713\n",
            "Episode 8000: Average Reward: -142.166\n",
            "Episode 9000: Average Reward: -158.246\n",
            "Episode 10000: Average Reward: -142.983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mz2kCfG_2otI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}